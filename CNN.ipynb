{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a51ebb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cbf49b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames extracted: 1248\n"
     ]
    }
   ],
   "source": [
    "video_path = 'C:/Users/HP/Zoomcamp/Datasets/nakawa_output.avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Frame extraction rate (every 2 seconds)\n",
    "extraction_rate = 2  \n",
    "frame_skip = int(fps * extraction_rate)\n",
    "\n",
    "frame_dir = 'video_frames'\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "count = 0\n",
    "frame_index = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if frame_index % frame_skip == 0:\n",
    "        frame_filename = os.path.join(frame_dir, f\"frame_{count:05d}.jpg\")\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        count += 1\n",
    "    \n",
    "    frame_index += 1\n",
    "\n",
    "cap.release()\n",
    "print(f\"Frames extracted: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70417456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully organized and split into train, validation, and test sets.\n"
     ]
    }
   ],
   "source": [
    "# Base directory where the exported data is stored\n",
    "base_dir = 'D:/Users HP/Downloads/cnn'\n",
    "images_dir = os.path.join(base_dir, 'JPEGImages')\n",
    "annotations_dir = os.path.join(base_dir, 'Annotations')\n",
    "\n",
    "# Create directories for train, validation, test sets\n",
    "data_splits = ['train', 'val', 'test']\n",
    "for split in data_splits:\n",
    "    os.makedirs(os.path.join(base_dir, split, 'Images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(base_dir, split, 'Annotations'), exist_ok=True)\n",
    "\n",
    "# Get a list of filenames without extension\n",
    "all_files = [os.path.splitext(f)[0] for f in os.listdir(images_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "# Split data into train, val, and test\n",
    "train_files, test_files = train_test_split(all_files, test_size=0.3, random_state=42)  # 70% training, 30% testing\n",
    "train_files, val_files = train_test_split(train_files, test_size=0.2, random_state=42)  # Splitting 20% of training for validation\n",
    "\n",
    "# Function to copy files\n",
    "def copy_files(files, src_folder_images, src_folder_annotations, dst_folder_images, dst_folder_annotations):\n",
    "    for fname in files:\n",
    "        shutil.copy(os.path.join(src_folder_images, fname + '.jpg'), os.path.join(dst_folder_images, fname + '.jpg'))\n",
    "        shutil.copy(os.path.join(src_folder_annotations, fname + '.xml'), os.path.join(dst_folder_annotations, fname + '.xml'))\n",
    "\n",
    "# Copy image and annotation files\n",
    "for file_set, split in [(train_files, 'train'), (val_files, 'val'), (test_files, 'test')]:\n",
    "    copy_files(file_set, images_dir, annotations_dir, os.path.join(base_dir, split, 'Images'), os.path.join(base_dir, split, 'Annotations'))\n",
    "\n",
    "print('Dataset successfully organized and split into train, validation, and test sets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09856768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Class Distribution: {'Motorcycle': 1622, 'Car': 2612}\n",
      "Validation Set Class Distribution: {'Motorcycle': 396, 'Car': 682}\n",
      "Test Set Class Analysis: {'Motorcycle': 873, 'Car': 1509}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def count_classes(directory):\n",
    "    class_counts = {}\n",
    "    annotation_dir = os.path.join(directory, 'Annotations')\n",
    "    \n",
    "    for filename in os.listdir(annotation_dir):\n",
    "        if filename.endswith('.xml'):\n",
    "            path = os.path.join(annotation_dir, filename)\n",
    "            tree = ET.parse(path)\n",
    "            root = tree.getroot()\n",
    "            for member in root.findall('object'):\n",
    "                class_name = member.find('name').text\n",
    "                if class_name in class_counts:\n",
    "                    class_counts[class_name] += 1\n",
    "                else:\n",
    "                    class_counts[class_name] = 1\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "# Path to your dataset directory\n",
    "base_dir = 'D:/Users HP/Downloads/cnn'\n",
    "\n",
    "# Count classes in each dataset split\n",
    "train_counts = count_classes(os.path.join(base_dir, 'train'))\n",
    "val_counts = count_classes(os.path.join(base_dir, 'val'))\n",
    "test_counts = count_classes(os.path.join(base_dir, 'test'))\n",
    "\n",
    "print(\"Training Set Class Distribution:\", train_counts)\n",
    "print(\"Validation Set Class Distribution:\", val_counts)\n",
    "print(\"Test Set Class Analysis:\", test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46cc1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data generator\n",
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, image_dir, annot_dir, batch_size=32, image_size=(240, 320), shuffle=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.annot_dir = annot_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.image_paths, self.labels = self.load_dataset()\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def load_dataset(self):\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        for annot_file in os.listdir(self.annot_dir):\n",
    "            if annot_file.endswith('.xml'):\n",
    "                tree = ET.parse(os.path.join(self.annot_dir, annot_file))\n",
    "                root = tree.getroot()\n",
    "                filename = root.find('filename').text\n",
    "                image_path = os.path.join(self.image_dir, filename)\n",
    "                for obj in root.findall('object'):\n",
    "                    label = obj.find('name').text\n",
    "                    if label == 'Motorcycle':\n",
    "                        labels.append(1)\n",
    "                    elif label == 'Car':\n",
    "                        labels.append(0)\n",
    "                    image_paths.append(image_path)\n",
    "        return image_paths, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_image_paths = self.image_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        images = np.array([np.array(Image.open(img_path).resize(self.image_size)) / 255.0 for img_path in batch_image_paths])\n",
    "        labels = np.array(batch_labels)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            temp = list(zip(self.image_paths, self.labels))\n",
    "            np.random.shuffle(temp)\n",
    "            self.image_paths, self.labels = zip(*temp)\n",
    "\n",
    "# Directories\n",
    "train_image_dir = 'D:/Users HP/Downloads/cnn/train/Images'\n",
    "train_annot_dir = 'D:/Users HP/Downloads/cnn/train/Annotations'\n",
    "val_image_dir = 'D:/Users HP/Downloads/cnn/val/Images'\n",
    "val_annot_dir = 'D:/Users HP/Downloads/cnn/val/Annotations'\n",
    "test_image_dir = 'D:/Users HP/Downloads/cnn/test/Images'\n",
    "test_annot_dir = 'D:/Users HP/Downloads/cnn/test/Annotations'\n",
    "\n",
    "# Data Generators\n",
    "train_generator = CustomDataGenerator(train_image_dir, train_annot_dir, batch_size=32, image_size=(240, 320), shuffle=True)\n",
    "val_generator = CustomDataGenerator(val_image_dir, val_annot_dir, batch_size=32, image_size=(240, 320), shuffle=False)\n",
    "test_generator = CustomDataGenerator(test_image_dir, test_annot_dir, batch_size=32, image_size=(240, 320), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e6324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b0cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">238</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">318</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">119</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">159</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">117</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">157</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">136192</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │      <span style=\"color: #00af00; text-decoration-color: #00af00\">17,432,704</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m238\u001b[0m, \u001b[38;5;34m318\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m119\u001b[0m, \u001b[38;5;34m159\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m117\u001b[0m, \u001b[38;5;34m157\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m78\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m76\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m136192\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │      \u001b[38;5;34m17,432,704\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,526,081</span> (66.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,526,081\u001b[0m (66.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,526,081</span> (66.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,526,081\u001b[0m (66.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 2s/step - accuracy: 0.5876 - loss: 1.0235 - val_accuracy: 0.6316 - val_loss: 0.6663\n",
      "Epoch 2/10\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 2s/step - accuracy: 0.6085 - loss: 0.6707 - val_accuracy: 0.6316 - val_loss: 0.6618\n",
      "Epoch 3/10\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 2s/step - accuracy: 0.6111 - loss: 0.6725 - val_accuracy: 0.6316 - val_loss: 0.6694\n",
      "Epoch 4/10\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 2s/step - accuracy: 0.6129 - loss: 0.6685 - val_accuracy: 0.6316 - val_loss: 0.6592\n",
      "Epoch 5/10\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 2s/step - accuracy: 0.6292 - loss: 0.6587 - val_accuracy: 0.6316 - val_loss: 0.6566\n",
      "Epoch 6/10\n",
      "\u001b[1m 73/132\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2:25\u001b[0m 2s/step - accuracy: 0.6098 - loss: 0.6609"
     ]
    }
   ],
   "source": [
    "#Basic CNN Model\n",
    "\n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "input_shape = (240, 320, 3)\n",
    "cnn_model = build_cnn_model(input_shape)\n",
    "cnn_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.summary()\n",
    "\n",
    "epochs = 10\n",
    "history = cnn_model.fit(train_generator, validation_data=val_generator, epochs=epochs)\n",
    "\n",
    "# Evaluating the model\n",
    "def evaluate_model(model, test_generator):\n",
    "    predictions = model.predict(test_generator)\n",
    "    y_true = np.concatenate([test_generator[i][1] for i in range(len(test_generator))])\n",
    "    y_pred = (predictions > 0.5).astype(int).reshape(-1)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cr = classification_report(y_true, y_pred, target_names=['Car', 'Motorcycle'])\n",
    "    return cm, cr\n",
    "\n",
    "cm, cr = evaluate_model(cnn_model, test_generator)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Classification Report:\\n\", cr)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccbdefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Function to preprocess image\n",
    "def preprocess_image(image, input_size):\n",
    "    image = cv2.resize(image, input_size)\n",
    "    image = img_to_array(image) / 255.0\n",
    "    return np.expand_dims(image, axis=0)\n",
    "\n",
    "# Function to annotate image\n",
    "def annotate_image(image, label, score, bbox):\n",
    "    annotated_image = image.copy()\n",
    "    x, y, w, h = bbox\n",
    "    if label == 1 and score > 0.5:  # Assuming 1 is the label for object of interest\n",
    "        cv2.rectangle(annotated_image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(annotated_image, f'{label} {score:.2f}', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "    return annotated_image\n",
    "\n",
    "# Load and preprocess input image\n",
    "input_image_path = 'D:/Users HP/Downloads/cnn/JPEGImages/frame_01158.jpg'\n",
    "input_image = np.array(Image.open(input_image_path))\n",
    "input_size = (240, 320)  # Adjust based on your model's input size\n",
    "preprocessed_image = preprocess_image(input_image, input_size)\n",
    "\n",
    "# Predict on the entire image\n",
    "prediction = cnn_model.predict(preprocessed_image)\n",
    "label = (prediction > 0.5).astype(int)[0][0]\n",
    "score = prediction[0][0]\n",
    "\n",
    "# Annotate the image\n",
    "bbox = (50, 50, 200, 200)  # Example bounding box, replace with actual region coordinates\n",
    "annotated_image = annotate_image(input_image, label, score, bbox)\n",
    "\n",
    "# Display the annotated image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(annotated_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7db172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
