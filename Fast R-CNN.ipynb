{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c083cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import selectivesearch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a934d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                │     <span style=\"color: #00af00; text-decoration-color: #00af00\">102,764,544</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                │      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,781,312</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,194</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │      \u001b[38;5;34m14,714,688\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                │     \u001b[38;5;34m102,764,544\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                │      \u001b[38;5;34m16,781,312\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │           \u001b[38;5;34m8,194\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">134,268,738</span> (512.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m134,268,738\u001b[0m (512.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">119,554,050</span> (456.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m119,554,050\u001b[0m (456.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m855s\u001b[0m 1s/step - accuracy: 0.4692 - loss: 34.0238 - val_accuracy: 0.5943 - val_loss: 0.6769\n",
      "Epoch 2/10\n",
      "\u001b[1m581/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m2:49\u001b[0m 1s/step - accuracy: 0.5957 - loss: 1.2649"
     ]
    }
   ],
   "source": [
    "# Function for selective search\n",
    "def selective_search(image):\n",
    "    _, regions = selectivesearch.selective_search(image, scale=500, sigma=0.9, min_size=10)\n",
    "    candidates = set()\n",
    "    for r in regions:\n",
    "        if r['rect'] in candidates:\n",
    "            continue\n",
    "        if r['size'] < 2000:\n",
    "            continue\n",
    "        x, y, w, h = r['rect']\n",
    "        if w == 0 or h == 0:\n",
    "            continue\n",
    "        candidates.add(r['rect'])\n",
    "    return candidates\n",
    "\n",
    "# Custom Dataset Class\n",
    "class FastRCNNDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_dir, annot_dir, batch_size=1, input_size=(224, 224), shuffle=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.annot_dir = annot_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.shuffle = shuffle\n",
    "        self.image_paths, self.labels = self.load_dataset()\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def load_dataset(self):\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        for annot_file in os.listdir(self.annot_dir):\n",
    "            if annot_file.endswith('.xml'):\n",
    "                tree = ET.parse(os.path.join(self.annot_dir, annot_file))\n",
    "                root = tree.getroot()\n",
    "                filename = root.find('filename').text\n",
    "                image_path = os.path.join(self.image_dir, filename)\n",
    "                image_paths.append(image_path)\n",
    "                for obj in root.findall('object'):\n",
    "                    label = obj.find('name').text\n",
    "                    bndbox = obj.find('bndbox')\n",
    "                    xmin = round(float(bndbox.find('xmin').text))\n",
    "                    ymin = round(float(bndbox.find('ymin').text))\n",
    "                    xmax = round(float(bndbox.find('xmax').text))\n",
    "                    ymax = round(float(bndbox.find('ymax').text))\n",
    "                    labels.append(1 if label == 'Motorcycle' else 0)  # Binary labels\n",
    "        return image_paths, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_paths = self.image_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        for img_path, label in zip(image_paths, self.labels[index * self.batch_size:(index + 1) * self.batch_size]):\n",
    "            image = np.array(Image.open(img_path).resize(self.input_size)) / 255.0\n",
    "            batch_images.append(image)\n",
    "            batch_labels.append(label)\n",
    "        return np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            temp = list(zip(self.image_paths, self.labels))\n",
    "            np.random.shuffle(temp)\n",
    "            self.image_paths, self.labels = zip(*temp)\n",
    "\n",
    "# Build the Fast R-CNN Model\n",
    "def build_fast_rcnn_model(input_shape):\n",
    "    vgg = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in vgg.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    input_image = Input(shape=input_shape)\n",
    "    features = vgg(input_image)\n",
    "    x = Flatten()(features)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(2, activation='softmax')(x)  # 2 classes: object/no-object\n",
    "\n",
    "    model = Model(inputs=input_image, outputs=output)\n",
    "    return model\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "fast_rcnn_model = build_fast_rcnn_model(input_shape)\n",
    "fast_rcnn_model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "fast_rcnn_model.summary()\n",
    "\n",
    "# Directories\n",
    "train_image_dir = 'D:/Users HP/Downloads/cnn/train/Images'\n",
    "train_annot_dir = 'D:/Users HP/Downloads/cnn/train/Annotations'\n",
    "val_image_dir = 'D:/Users HP/Downloads/cnn/val/Images'\n",
    "val_annot_dir = 'D:/Users HP/Downloads/cnn/val/Annotations'\n",
    "test_image_dir = 'D:/Users HP/Downloads/cnn/test/Images'\n",
    "test_annot_dir = 'D:/Users HP/Downloads/cnn/test/Annotations'\n",
    "\n",
    "# Data Generators\n",
    "train_generator = FastRCNNDataset(train_image_dir, train_annot_dir, batch_size=1, input_size=(224, 224), shuffle=True)\n",
    "val_generator = FastRCNNDataset(val_image_dir, val_annot_dir, batch_size=1, input_size=(224, 224), shuffle=False)\n",
    "test_generator = FastRCNNDataset(test_image_dir, test_annot_dir, batch_size=1, input_size=(224, 224), shuffle=False)\n",
    "\n",
    "# Train the Fast R-CNN Model\n",
    "epochs = 10\n",
    "history = fast_rcnn_model.fit(train_generator, validation_data=val_generator, epochs=epochs)\n",
    "\n",
    "# Evaluate the Fast R-CNN Model\n",
    "def evaluate_fast_rcnn_model(model, generator):\n",
    "    predictions = model.predict(generator)\n",
    "    y_true = np.concatenate([generator[i][1] for i in range(len(generator))])\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cr = classification_report(y_true, y_pred, target_names=['Car', 'Motorcycle'])\n",
    "    return cm, cr\n",
    "\n",
    "cm, cr = evaluate_fast_rcnn_model(fast_rcnn_model, test_generator)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Classification Report:\\n\", cr)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess image and regions\n",
    "def preprocess_regions(image, regions, input_size):\n",
    "    region_images = []\n",
    "    region_coords = []\n",
    "    for (x, y, w, h) in regions:\n",
    "        region = image[y:y+h, x:x+w]\n",
    "        region = cv2.resize(region, input_size)\n",
    "        region = region / 255.0\n",
    "        region_images.append(region)\n",
    "        region_coords.append((x, y, w, h))\n",
    "    return np.array(region_images), region_coords\n",
    "\n",
    "# Function to annotate image\n",
    "def annotate_image(image, regions, labels, scores):\n",
    "    annotated_image = image.copy()\n",
    "    for (x, y, w, h), label, score in zip(regions, labels, scores):\n",
    "        if label == 1 and score > 0.5:  # Assuming 1 is the label for object of interest\n",
    "            cv2.rectangle(annotated_image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            cv2.putText(annotated_image, f'{label} {score:.2f}', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "    return annotated_image\n",
    "\n",
    "# Load and preprocess input image\n",
    "input_image_path = 'D:/Users HP/Downloads/cnn/JPEGImages/frame_01158.jpg'\n",
    "input_image = np.array(Image.open(input_image_path))\n",
    "regions = selective_search(input_image)\n",
    "preprocessed_regions, region_coords = preprocess_regions(input_image, regions, (224, 224))\n",
    "\n",
    "# Predict on regions\n",
    "predictions = model.predict(preprocessed_regions)\n",
    "labels = np.argmax(predictions, axis=1)\n",
    "scores = np.max(predictions, axis=1)\n",
    "\n",
    "# Annotate image\n",
    "annotated_image = annotate_image(input_image, region_coords, labels, scores)\n",
    "\n",
    "# Display the annotated image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(annotated_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce84aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
